{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1SRD1R9-FmNORKx_mqqF2fVy1c0Xak2Fv","authorship_tag":"ABX9TyPflqrKbuYeWY5niqFU61YV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kOXXSyzOuGAy","executionInfo":{"status":"ok","timestamp":1724575806670,"user_tz":-330,"elapsed":3385,"user":{"displayName":"sai vignesh","userId":"12278521234982387543"}},"outputId":"569d0596-55e6-4844-f54e-b2a0aa259408"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.7.4)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"]}],"source":["!pip install kaggle\n"]},{"cell_type":"code","source":["from google.colab import files\n","files.upload()  # Upload the kaggle.json file here\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84},"id":"XuGE52RTun_z","executionInfo":{"status":"ok","timestamp":1724575838924,"user_tz":-330,"elapsed":20075,"user":{"displayName":"sai vignesh","userId":"12278521234982387543"}},"outputId":"7de1e9b5-ea64-4ab2-efa4-d53094204454"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-541d9cb1-9e13-4674-b558-430a81e81aed\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-541d9cb1-9e13-4674-b558-430a81e81aed\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'kaggle.json': b'{\"username\":\"vignesh999\",\"key\":\"da09f7ffb63a8de32a6b20046f2915a6\"}'}"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["!mkdir -p ~/.kaggle\n","!mv kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n"],"metadata":{"id":"P_dHQCE4uvsC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Download the dataset to a specific folder in Google Drive\n","!kaggle datasets download -d sriramr/fruits-fresh-and-rotten-for-classification -p /content/drive/MyDrive/RFD\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ToL-abD_uyuI","executionInfo":{"status":"ok","timestamp":1724575964693,"user_tz":-330,"elapsed":80957,"user":{"displayName":"sai vignesh","userId":"12278521234982387543"}},"outputId":"7b8e5382-a613-42b1-8e2b-a6532763bb44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Dataset URL: https://www.kaggle.com/datasets/sriramr/fruits-fresh-and-rotten-for-classification\n","License(s): unknown\n","Downloading fruits-fresh-and-rotten-for-classification.zip to /content/drive/MyDrive/RFD\n","100% 3.58G/3.58G [01:05<00:00, 66.3MB/s]\n","100% 3.58G/3.58G [01:05<00:00, 58.4MB/s]\n"]}]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/RFD/fruits-fresh-and-rotten-for-classification.zip -d /content/drive/MyDrive/RFD/\n"],"metadata":{"collapsed":true,"id":"SZKto10Zvbml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow opencv-python\n"],"metadata":{"collapsed":true,"id":"s5djrwnGxLHf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","\n","# Path to your dataset\n","dataset_path = '/content/drive/MyDrive/RFD/dataset/dataset/'\n","\n","# Image size\n","IMG_SIZE = 128\n","\n","# Function to load and preprocess images from all subfolders\n","def load_images_from_subfolders(base_folder):\n","    images = []\n","    labels = []\n","    class_labels = {'freshapples': 0, 'freshbanana': 1, 'freshoranges': 2,\n","                    'rottenapples': 3, 'rottenbanana': 4, 'rottenoranges': 5}\n","\n","    for folder_name in os.listdir(base_folder):\n","        folder_path = os.path.join(base_folder, folder_name)\n","        if os.path.isdir(folder_path):\n","            label = class_labels[folder_name]\n","            for filename in os.listdir(folder_path):\n","                img = cv2.imread(os.path.join(folder_path, filename))\n","                if img is not None:\n","                    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n","                    images.append(img)\n","                    labels.append(label)\n","\n","    return np.array(images), np.array(labels)\n","\n","# Load training data\n","train_images, train_labels = load_images_from_subfolders(os.path.join(dataset_path, 'train'))\n","\n","# Load testing data\n","test_images, test_labels = load_images_from_subfolders(os.path.join(dataset_path, 'test'))\n","\n","# Data Augmentation\n","train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow(train_images, train_labels, batch_size=32)\n","test_generator = test_datagen.flow(test_images, test_labels, batch_size=32)\n"],"metadata":{"id":"6NS0auJDxPeO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","\n","model = Sequential([\n","    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n","    MaxPooling2D(2, 2),\n","    Conv2D(64, (3, 3), activation='relu'),\n","    MaxPooling2D(2, 2),\n","    Conv2D(128, (3, 3), activation='relu'),\n","    MaxPooling2D(2, 2),\n","    Flatten(),\n","    Dense(512, activation='relu'),\n","    Dropout(0.5),\n","    Dense(6, activation='softmax')  # Change to 6 for multiclass classification\n","])\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n"],"metadata":{"id":"SpZV5IlG0KT7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(train_generator, epochs=1, validation_data=test_generator)\n","model.save('fruit_ripening_model.h5')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQ6shk2q0QSM","executionInfo":{"status":"ok","timestamp":1724577737354,"user_tz":-330,"elapsed":85187,"user":{"displayName":"sai vignesh","userId":"12278521234982387543"}},"outputId":"2759f394-210c-4f3b-92e7-d6e946bbdf37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 186ms/step - accuracy: 0.5549 - loss: 1.1706 - val_accuracy: 0.8781 - val_loss: 0.3401\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]}]},{"cell_type":"code","source":["!apt-get install xvfb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fv-0Rrlk4sIf","executionInfo":{"status":"ok","timestamp":1724578759253,"user_tz":-330,"elapsed":12532,"user":{"displayName":"sai vignesh","userId":"12278521234982387543"}},"outputId":"7217b99a-efeb-46fa-86f2-e6bde76e24cb"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 7,813 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.11 [28.6 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.11 [863 kB]\n","Fetched 7,813 kB in 1s (8,271 kB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 123595 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.11_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.11_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n"]}]},{"source":["import tkinter as tk\n","from tkinter import filedialog\n","import cv2\n","import numpy as np\n","from tensorflow.keras.models import load_model\n","from PIL import Image\n","\n","# Load the trained model\n","model = load_model('fruit_ripening_model.h5')\n","\n","# Define image size\n","IMG_SIZE = 128\n","\n","def process_image(image_path):\n","    # Open the image and convert to RGB\n","    image = Image.open(image_path).convert('RGB')\n","\n","    # Convert the image to a numpy array and resize it\n","    image_np = np.array(image)\n","    image_resized = cv2.resize(image_np, (IMG_SIZE, IMG_SIZE))\n","    image_resized = np.expand_dims(image_resized, axis=0) / 255.0\n","\n","    # Make a prediction\n","    prediction = model.predict(image_resized)\n","    class_label = np.argmax(prediction[0])  # Assuming 6 classes\n","\n","    return class_label\n","\n","def upload_image():\n","    # Create a Tkinter root window\n","    root = tk.Tk()\n","    root.withdraw()  # Hide the root window\n","\n","    # Open a file dialog to select an image file\n","    file_path = filedialog.askopenfilename(\n","        title=\"Select an Image\",\n","        filetypes=[(\"Image Files\", \"*.png;*.jpg;*.jpeg\")]\n","    )\n","\n","    if file_path:\n","        # Process the image and print the prediction\n","        class_label = process_image(file_path)\n","        print(f'Predicted class: {class_label}')\n","    else:\n","        print(\"No file selected.\")\n","\n","if __name__ == \"__main__\":\n","    upload_image()\n"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"R36pwGBx5oHf","executionInfo":{"status":"error","timestamp":1724578865457,"user_tz":-330,"elapsed":9366,"user":{"displayName":"sai vignesh","userId":"12278521234982387543"}},"outputId":"97353829-5917-414f-bf36-45544e9ba386"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"error","ename":"TclError","evalue":"no display name and no $DISPLAY environment variable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-50890cf99683>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mupload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-50890cf99683>\u001b[0m in \u001b[0;36mupload_image\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Create a Tkinter root window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Hide the root window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2297\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"]}]},{"source":["import tkinter as tk\n","from tkinter import filedialog\n","import cv2\n","import numpy as np\n","from tensorflow.keras.models import load_model\n","from PIL import Image\n","import os\n","\n","# Load the trained model\n","model = load_model('fruit_ripening_model.h5')\n","\n","# Define image size\n","IMG_SIZE = 128\n","\n","def process_image(image_path):\n","    # Open the image and convert to RGB\n","    image = Image.open(image_path).convert('RGB')\n","\n","    # Convert the image to a numpy array and resize it\n","    image_np = np.array(image)\n","    image_resized = cv2.resize(image_np, (IMG_SIZE, IMG_SIZE))\n","    image_resized = np.expand_dims(image_resized, axis=0) / 255.0\n","\n","    # Make a prediction\n","    prediction = model.predict(image_resized)\n","    class_label = np.argmax(prediction[0])  # Assuming 6 classes\n","\n","    return class_label\n","\n","def upload_image():\n","    # Create a virtual display\n","    if os.environ.get(\"DISPLAY\", \"\") == \"\":\n","        print(\"no display found. Using :0.0\")\n","        os.environ.__setitem__(\"DISPLAY\", \":0.0\")\n","\n","    # Create a Tkinter root window\n","    root = tk.Tk()\n","    root.withdraw()  # Hide the root window\n","\n","    # Open a file dialog to select an image file\n","    file_path = filedialog.askopenfilename(\n","        title=\"Select an Image\",\n","        filetypes=[(\"Image Files\", \"*.png;*.jpg;*.jpeg\")]\n","    )\n","\n","    if file_path:\n","        # Process the image and print the prediction\n","        class_label = process_image(file_path)\n","        print(f'Predicted class: {class_label}')\n","    else:\n","        print(\"No file selected.\")\n","\n","if __name__ == \"__main__\":\n","    upload_image()"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"id":"fGqM_Knx6bB4","executionInfo":{"status":"error","timestamp":1724578904414,"user_tz":-330,"elapsed":439,"user":{"displayName":"sai vignesh","userId":"12278521234982387543"}},"outputId":"19237745-b556-4604-f26c-13258b665573"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["no display found. Using :0.0\n"]},{"output_type":"error","ename":"TclError","evalue":"couldn't connect to display \":0.0\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b95992850727>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mupload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-b95992850727>\u001b[0m in \u001b[0;36mupload_image\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Create a Tkinter root window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Hide the root window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2297\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTclError\u001b[0m: couldn't connect to display \":0.0\""]}]},{"source":["import tkinter as tk\n","from tkinter import filedialog\n","import cv2\n","import numpy as np\n","from tensorflow.keras.models import load_model\n","from PIL import Image\n","import os\n","\n","# Load the trained model\n","model = load_model('fruit_ripening_model.h5')\n","\n","# Define image size\n","IMG_SIZE = 128\n","\n","def process_image(image_path):\n","    # Open the image and convert to RGB\n","    image = Image.open(image_path).convert('RGB')\n","\n","    # Convert the image to a numpy array and resize it\n","    image_np = np.array(image)\n","    image_resized = cv2.resize(image_np, (IMG_SIZE, IMG_SIZE))\n","    image_resized = np.expand_dims(image_resized, axis=0) / 255.0\n","\n","    # Make a prediction\n","    prediction = model.predict(image_resized)\n","    class_label = np.argmax(prediction[0])  # Assuming 6 classes\n","\n","    return class_label\n","\n","def upload_image():\n","    # Use try-except block to handle the exception\n","    try:\n","        # Create a Tkinter root window\n","        root = tk.Tk()\n","        root.withdraw()  # Hide the root window\n","\n","        # Open a file dialog to select an image file\n","        file_path = filedialog.askopenfilename(\n","            title=\"Select an Image\",\n","            filetypes=[(\"Image Files\", \"*.png;*.jpg;*.jpeg\")]\n","        )\n","\n","        if file_path:\n","            # Process the image and print the prediction\n","            class_label = process_image(file_path)\n","            print(f'Predicted class: {class_label}')\n","        else:\n","            print(\"No file selected.\")\n","    except tk.TclError:\n","        print(\"Tkinter GUI failed. You might be running in a headless environment.\")\n","        print(\"Consider using a different method for image selection.\")\n","\n","if __name__ == \"__main__\":\n","    upload_image()"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JF1VH2ln6o1R","executionInfo":{"status":"ok","timestamp":1724578961289,"user_tz":-330,"elapsed":672,"user":{"displayName":"sai vignesh","userId":"12278521234982387543"}},"outputId":"45d32964-c3f1-45d9-eff7-b8b996bf369e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["Tkinter GUI failed. You might be running in a headless environment.\n","Consider using a different method for image selection.\n"]}]},{"source":["import cv2\n","import numpy as np\n","from tensorflow.keras.models import load_model\n","from PIL import Image\n","\n","# Load the trained model\n","model = load_model('fruit_ripening_model.h5')\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Recompile the model\n","\n","# Define image size\n","IMG_SIZE = 128\n","\n","def process_image(image_path):\n","    # Open the image and convert to RGB\n","    image = Image.open(image_path).convert('RGB')\n","\n","    # Convert the image to a numpy array and resize it\n","    image_np = np.array(image)\n","    image_resized = cv2.resize(image_np, (IMG_SIZE, IMG_SIZE))\n","    image_resized = np.expand_dims(image_resized, axis=0) / 255.0\n","\n","    # Make a prediction\n","    prediction = model.predict(image_resized)\n","    class_label = np.argmax(prediction[0])  # Assuming 6 classes\n","\n","    return class_label\n","\n","def upload_image():\n","    # Get the image path from the user\n","    file_path = input(\"Enter the path to your image: \")\n","\n","    if file_path:\n","        # Process the image and print the prediction\n","        class_label = process_image(file_path)\n","        print(f'Predicted class: {class_label}')\n","    else:\n","        print(\"No file path entered.\")\n","\n","if __name__ == \"__main__\":\n","    upload_image()"],"cell_type":"code","metadata":{"id":"tMNW7tyz6yDB","outputId":"e0ffffbd-f3e3-42ed-c22b-41c379aa5d60","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}]},{"source":["!pip install pyvirtualdisplay"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sfYWhk8u5oll","executionInfo":{"status":"ok","timestamp":1724578701201,"user_tz":-330,"elapsed":3171,"user":{"displayName":"sai vignesh","userId":"12278521234982387543"}},"outputId":"5515daee-5397-4f58-a5b5-3099b7426572"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n","Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n"]}]},{"cell_type":"code","source":["!pip install opencv-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fn8_xYaa3pIV","executionInfo":{"status":"ok","timestamp":1724578186047,"user_tz":-330,"elapsed":6172,"user":{"displayName":"sai vignesh","userId":"12278521234982387543"}},"outputId":"9630f23b-06c0-4030-ea63-8f8e91808d29"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()\n","\n","import cv2\n","import numpy as np\n","from PIL import Image\n","from io import BytesIO\n","\n","for filename in uploaded.keys():\n","    # Load and convert the image to RGB\n","    image = Image.open(BytesIO(uploaded[filename])).convert('RGB')\n","    image_np = np.array(image)\n","\n","    # Resize and preprocess the image\n","    image_resized = cv2.resize(image_np, (IMG_SIZE, IMG_SIZE))\n","    image_resized = np.expand_dims(image_resized, axis=0) / 255.0\n","\n","    # Make a prediction\n","    prediction = model.predict(image_resized)\n","    print(\"Prediction:\", prediction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"uxk73AWf2meB","executionInfo":{"status":"ok","timestamp":1724578348606,"user_tz":-330,"elapsed":18381,"user":{"displayName":"sai vignesh","userId":"12278521234982387543"}},"outputId":"48aad932-b9d6-40dc-d527-39e514b4a9f2"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-a5ed0502-eee8-4911-901c-16dcb7d8eb37\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-a5ed0502-eee8-4911-901c-16dcb7d8eb37\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Keep it simple.png to Keep it simple (1).png\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 673ms/step\n","Prediction: [[1.4065854e-12 4.7610506e-08 1.7168195e-12 1.7954470e-05 9.9998081e-01\n","  1.1500632e-06]]\n"]}]}]}